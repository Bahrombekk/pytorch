{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IgbaZmSmsKev"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1mchJuwfzviiM0nA8T3m+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bahrombekk/pytorch/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG13_BN   modeli"
      ],
      "metadata": {
        "id": "x9kYLFt6FCwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytjjNPrMD8R4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import vgg13_bn\n",
        "from torchvision.models import vgg19_bn,vit_h_14\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,DataLoader,random_split\n",
        "import pathlib\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG13_BN   modeli"
      ],
      "metadata": {
        "id": "IgbaZmSmsKev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -O tezyordam_in_polis.zip https://www.dropbox.com/s/zbrbvyj6la2jivv/Tez_in_polis.zip?dl=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcT_wGssGUsH",
        "outputId": "a3ccef6c-efda-470a-d891-d5f03226fea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-12 15:11:57--  https://www.dropbox.com/s/zbrbvyj6la2jivv/Tez_in_polis.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.69.18, 2620:100:6025:18::a27d:4512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.69.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/zbrbvyj6la2jivv/Tez_in_polis.zip [following]\n",
            "--2023-07-12 15:11:58--  https://www.dropbox.com/s/raw/zbrbvyj6la2jivv/Tez_in_polis.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com/cd/0/inline/B_vK7doUdlUoJSGV2jkNi1E5SuLe1mimuh2SJaxNxGv8kRyZ6a87o_4Viv7xuJZT-MoXqK0I5XFysW2H6ozVYG4U6G7lTHmHGMhIfxcW4fWXeTlOglsxpKszfpvhOJfmKTpXjJ7smrryLNNDNnVD52BArEukk4iS7bLyLwYK-3Da3Q/file# [following]\n",
            "--2023-07-12 15:11:59--  https://uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com/cd/0/inline/B_vK7doUdlUoJSGV2jkNi1E5SuLe1mimuh2SJaxNxGv8kRyZ6a87o_4Viv7xuJZT-MoXqK0I5XFysW2H6ozVYG4U6G7lTHmHGMhIfxcW4fWXeTlOglsxpKszfpvhOJfmKTpXjJ7smrryLNNDNnVD52BArEukk4iS7bLyLwYK-3Da3Q/file\n",
            "Resolving uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com (uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com (uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B_su6uZjS37P9LFnLKHgT3KkTM5fblQTLDnnG3lJQo4cocF-yJE3WZ0qKXwj975d8lwF2-SdGKDfhmUeKwI7cTkvoJOPBr5wcB3C3RwAS2xA41GUZ1n34iApqhhdGmFHQHWPTrHfA5ZbZwhDZqhAmbCibuvEeLroAoSV3DWnlEVk4dVfAb_fD4CqpA8NqOU8B1YK6XeRh3h9g3letWZ0g0zG3mI2Osei0S5YPv-SkqC-58R9aqHMdzF6z9lKNNgIowSsGYxyqhX6o7aVvV-fEfxIbnT4eD44CmgEbwA4cnHNoUNIh02jGmqDjQzKbtcqsBXzXcxp80uVOJpCOZEhhrFHnuOElStv3hByp7gj4nKIhN5H-ZsDN2iXjHNFJbfEqp1R2zEj89noyKZ9weBAi0UuP-hqHoFMfWX4F7txTDXbFQ/file [following]\n",
            "--2023-07-12 15:11:59--  https://uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com/cd/0/inline2/B_su6uZjS37P9LFnLKHgT3KkTM5fblQTLDnnG3lJQo4cocF-yJE3WZ0qKXwj975d8lwF2-SdGKDfhmUeKwI7cTkvoJOPBr5wcB3C3RwAS2xA41GUZ1n34iApqhhdGmFHQHWPTrHfA5ZbZwhDZqhAmbCibuvEeLroAoSV3DWnlEVk4dVfAb_fD4CqpA8NqOU8B1YK6XeRh3h9g3letWZ0g0zG3mI2Osei0S5YPv-SkqC-58R9aqHMdzF6z9lKNNgIowSsGYxyqhX6o7aVvV-fEfxIbnT4eD44CmgEbwA4cnHNoUNIh02jGmqDjQzKbtcqsBXzXcxp80uVOJpCOZEhhrFHnuOElStv3hByp7gj4nKIhN5H-ZsDN2iXjHNFJbfEqp1R2zEj89noyKZ9weBAi0UuP-hqHoFMfWX4F7txTDXbFQ/file\n",
            "Reusing existing connection to uc9b2c28f8b93f3e752ebb51a592.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25138312 (24M) [application/zip]\n",
            "Saving to: ‘tezyordam_in_polis.zip’\n",
            "\n",
            "tezyordam_in_polis. 100%[===================>]  23.97M  13.7MB/s    in 1.7s    \n",
            "\n",
            "2023-07-12 15:12:01 (13.7 MB/s) - ‘tezyordam_in_polis.zip’ saved [25138312/25138312]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tezyordam_in_polis.zip"
      ],
      "metadata": {
        "id": "_Ccco_mfoSoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "9sj_-t6poV_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize(size=(150,150)),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "ZuFQDAKcoZOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tez_in_pol_Dataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        self.path_list = list(path.glob('*/*.jpg'))\n",
        "        self.transform = transform\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(self.path_list[index])\n",
        "        label = (self.path_list[index]).parts[-2]\n",
        "        label = 0 if 'Polis' in label else 1\n",
        "        if transform != None:\n",
        "            img = transform(img)\n",
        "        return img, int(label)\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "metadata": {
        "id": "wRzJ8eU1ob8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = pathlib.Path('/content/Tez_in_polis/tren')\n",
        "test_path = pathlib.Path('/content/Tez_in_polis/test')"
      ],
      "metadata": {
        "id": "Y7etYzBkofmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Tez_in_pol_Dataset(train_path, transform)\n",
        "test_dataset = Tez_in_pol_Dataset(test_path, transform)"
      ],
      "metadata": {
        "id": "AOKEgqYDoh-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = list(train_path.glob('*/*.jpg'))\n",
        "path_test=list(test_path.glob('*/*.jpg'))"
      ],
      "metadata": {
        "id": "kqVz2SKdokBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_data = DataLoader(test_dataset, batch_size=5000, shuffle=True)"
      ],
      "metadata": {
        "id": "FvzylgAdomSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "My_model = vgg13_bn()\n",
        "My_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcodbJ6Mop3H",
        "outputId": "c68531c4-199d-4d1a-b0f3-65c4600216b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU(inplace=True)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): ReLU(inplace=True)\n",
              "    (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "My_model.fc =  torch.nn.Linear(in_features=4096,out_features=2)"
      ],
      "metadata": {
        "id": "QjzmwAIvoyy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(My_model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "ohstR3x6pJ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15"
      ],
      "metadata": {
        "id": "zUG2mzJlpMlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataloader, test_dataloader, criterion, optimizer, epochs=3):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.history = {'loss':[],'acc':[],'val_loss':[],'val_acc':[]}\n",
        "    def train_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.train_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.train_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            sum_loss += loss.item()\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "        train_loss = sum_loss/n\n",
        "        train_accuracy = sum_accuracy/n\n",
        "        self.history['loss'].append(train_loss)\n",
        "        self.history['acc'].append(train_accuracy)\n",
        "        return train_loss, train_accuracy\n",
        "    def validation_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.test_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.test_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_loss += loss.item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "        val_loss = sum_loss/n\n",
        "        val_accuracy = sum_accuracy/n\n",
        "        self.history['val_loss'].append(val_loss)\n",
        "        self.history['val_acc'].append(val_accuracy)\n",
        "        return val_loss, val_accuracy\n",
        "    def train(self):\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc = self.train_loop()\n",
        "            val_loss, val_acc = self.validation_loop()\n",
        "            print()\n",
        "            print(f'Epoch[{epoch+1}/{num_epochs}] \\t train_loss: {train_loss:.5f}, train_acc: {train_acc:.2f} \\t val_loss: {val_loss:.5f} \\t val_acc: {val_acc:.2}')\n"
      ],
      "metadata": {
        "id": "fZKjDhSipSyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = My_model.to(device),\n",
        "    train_dataloader = train_data,\n",
        "    test_dataloader = test_data,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "9A9Wf9ROpVs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMs0OJkbpYGM",
        "outputId": "67d1786d-3085-4b41-a824-54125f55a590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:26<00:00,  1.81it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[1/15] \t train_loss: 9.30404, train_acc: 0.58 \t val_loss: 0.69987 \t val_acc: 0.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.48it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[2/15] \t train_loss: 0.59558, train_acc: 0.66 \t val_loss: 0.64664 \t val_acc: 0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:20<00:00,  2.38it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[3/15] \t train_loss: 0.48244, train_acc: 0.80 \t val_loss: 0.27893 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:20<00:00,  2.37it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[4/15] \t train_loss: 0.34703, train_acc: 0.88 \t val_loss: 0.59555 \t val_acc: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[5/15] \t train_loss: 0.25867, train_acc: 0.91 \t val_loss: 0.61349 \t val_acc: 0.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.46it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[6/15] \t train_loss: 0.21166, train_acc: 0.93 \t val_loss: 0.38353 \t val_acc: 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.44it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[7/15] \t train_loss: 0.17674, train_acc: 0.94 \t val_loss: 0.35197 \t val_acc: 0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.41it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[8/15] \t train_loss: 0.09777, train_acc: 0.97 \t val_loss: 0.54750 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.41it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[9/15] \t train_loss: 0.12372, train_acc: 0.96 \t val_loss: 0.46866 \t val_acc: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.43it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[10/15] \t train_loss: 0.10022, train_acc: 0.97 \t val_loss: 0.21698 \t val_acc: 0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.44it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[11/15] \t train_loss: 0.08744, train_acc: 0.97 \t val_loss: 0.44992 \t val_acc: 0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[12/15] \t train_loss: 0.12785, train_acc: 0.96 \t val_loss: 0.78197 \t val_acc: 0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[13/15] \t train_loss: 0.13860, train_acc: 0.95 \t val_loss: 0.67029 \t val_acc: 0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[14/15] \t train_loss: 0.13250, train_acc: 0.95 \t val_loss: 0.71168 \t val_acc: 0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[15/15] \t train_loss: 0.06528, train_acc: 0.98 \t val_loss: 0.44880 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img,test_label=next(iter(test_data))\n",
        "pred=My_model(test_img.to(device))\n",
        "pred=pred.argmax(axis=1)\n",
        "pred = pred.detach().cpu()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( test_label)\n",
        "cm=confusion_matrix(test_label,pred)\n",
        "print(cm)\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm,annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "GVxfiRvarmC9",
        "outputId": "c67bc701-26dc-43a5-a0a1-abfc5c622a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0])\n",
            "[[26  4]\n",
            " [ 0 30]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG19_BN"
      ],
      "metadata": {
        "id": "2jA7Y4YytWqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_model2 = vgg19_bn()\n",
        "My_model2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK1oizditNtw",
        "outputId": "5447cde0-4d9a-4517-8302-4793c7c62721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (38): ReLU(inplace=True)\n",
              "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (45): ReLU(inplace=True)\n",
              "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (48): ReLU(inplace=True)\n",
              "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (51): ReLU(inplace=True)\n",
              "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "My_model2.classifier[3]=torch.nn.Linear(in_features=4096,out_features=2)"
      ],
      "metadata": {
        "id": "ZawQVdeWtftb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bH43cm72uIQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize(size=(150,150)),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "6vy1BQw6uMcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tez_in_pol_Dataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        self.path_list = list(path.glob('*/*.jpg'))\n",
        "        self.transform = transform\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(self.path_list[index])\n",
        "        label = (self.path_list[index]).parts[-2]\n",
        "        label = 0 if 'Polis' in label else 1\n",
        "        if transform != None:\n",
        "            img = transform(img)\n",
        "        return img, int(label)\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "metadata": {
        "id": "yuB4YKRSuQuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = pathlib.Path('/content/Tez_in_polis/tren')\n",
        "test_path = pathlib.Path('/content/Tez_in_polis/test')"
      ],
      "metadata": {
        "id": "COEbCm_2uU-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Tez_in_pol_Dataset(train_path, transform)\n",
        "test_dataset = Tez_in_pol_Dataset(test_path, transform)"
      ],
      "metadata": {
        "id": "YH7Zi9VjuZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = list(train_path.glob('*/*.jpg'))\n",
        "path_test=list(test_path.glob('*/*.jpg'))"
      ],
      "metadata": {
        "id": "OsN6nEtRudGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_data = DataLoader(test_dataset, batch_size=5000, shuffle=True)"
      ],
      "metadata": {
        "id": "KNKePWDduhKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(My_model2.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "2G2JP7Lpt_Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15"
      ],
      "metadata": {
        "id": "2xFe2LvzuCHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataloader, test_dataloader, criterion, optimizer, epochs=3):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.history = {'loss':[],'acc':[],'val_loss':[],'val_acc':[]}\n",
        "    def train_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.train_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.train_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            sum_loss += loss.item()\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "        train_loss = sum_loss/n\n",
        "        train_accuracy = sum_accuracy/n\n",
        "        self.history['loss'].append(train_loss)\n",
        "        self.history['acc'].append(train_accuracy)\n",
        "        return train_loss, train_accuracy\n",
        "    def validation_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.test_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.test_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_loss += loss.item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "        val_loss = sum_loss/n\n",
        "        val_accuracy = sum_accuracy/n\n",
        "        self.history['val_loss'].append(val_loss)\n",
        "        self.history['val_acc'].append(val_accuracy)\n",
        "        return val_loss, val_accuracy\n",
        "    def train(self):\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc = self.train_loop()\n",
        "            val_loss, val_acc = self.validation_loop()\n",
        "            print()\n",
        "            print(f'Epoch[{epoch+1}/{num_epochs}] \\t train_loss: {train_loss:.5f}, train_acc: {train_acc:.2f} \\t val_loss: {val_loss:.5f} \\t val_acc: {val_acc:.2}')\n"
      ],
      "metadata": {
        "id": "0fSsRXFpuqAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1 = Trainer(\n",
        "    model = My_model.to(device),\n",
        "    train_dataloader = train_data,\n",
        "    test_dataloader = test_data,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "nz5X8x2Kut-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYnA25HRuw2R",
        "outputId": "126fcfda-f06e-4757-fc6c-94089b66c75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[1/15] \t train_loss: 0.08943, train_acc: 0.97 \t val_loss: 0.42906 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.60it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[2/15] \t train_loss: 0.07516, train_acc: 0.98 \t val_loss: 0.34053 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.70it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[3/15] \t train_loss: 0.08466, train_acc: 0.97 \t val_loss: 0.28262 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.68it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[4/15] \t train_loss: 0.07699, train_acc: 0.97 \t val_loss: 0.31574 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.70it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[5/15] \t train_loss: 0.07310, train_acc: 0.97 \t val_loss: 0.25799 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.64it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[6/15] \t train_loss: 0.08023, train_acc: 0.97 \t val_loss: 0.32314 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.64it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[7/15] \t train_loss: 0.08190, train_acc: 0.98 \t val_loss: 0.29517 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.68it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[8/15] \t train_loss: 0.09515, train_acc: 0.97 \t val_loss: 0.34552 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[9/15] \t train_loss: 0.08583, train_acc: 0.98 \t val_loss: 0.50745 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[10/15] \t train_loss: 0.07415, train_acc: 0.98 \t val_loss: 0.43088 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[11/15] \t train_loss: 0.08741, train_acc: 0.97 \t val_loss: 0.30313 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[12/15] \t train_loss: 0.07658, train_acc: 0.98 \t val_loss: 0.34852 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[13/15] \t train_loss: 0.07190, train_acc: 0.98 \t val_loss: 0.30813 \t val_acc: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:18<00:00,  2.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[14/15] \t train_loss: 0.08123, train_acc: 0.97 \t val_loss: 0.24045 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:17<00:00,  2.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch[15/15] \t train_loss: 0.07731, train_acc: 0.97 \t val_loss: 0.34200 \t val_acc: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img,test_label=next(iter(test_data))\n",
        "pred=My_model(test_img.to(device))\n",
        "pred=pred.argmax(axis=1)\n",
        "pred = pred.detach().cpu()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( test_label)\n",
        "cm=confusion_matrix(test_label,pred)\n",
        "print(cm)\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm,annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "77ppdaYvvRXq",
        "outputId": "57071cce-de30-4efb-b68d-c38d620da8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
            "[[26  4]\n",
            " [ 0 30]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT_H_14"
      ],
      "metadata": {
        "id": "OJves0dqw6OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_model3=vit_h_14()\n",
        "My_model3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M-KXYpUw4-v",
        "outputId": "f739f945-89d0-4a79-a6f5-10f7a28a65ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_12): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_13): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_14): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_15): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_16): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_17): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_18): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_19): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_20): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_21): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_22): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_23): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_24): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_25): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_26): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_27): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_28): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_29): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_30): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_31): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "My_model3.heads.head=torch.nn.Linear(in_features=1280,out_features=2)\n",
        "My_model3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCDNb8tLxkki",
        "outputId": "556f5202-45b3-465f-d2bb-6f28c6a5e3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_12): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_13): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_14): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_15): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_16): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_17): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_18): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_19): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_20): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_21): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_22): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_23): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_24): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_25): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_26): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_27): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_28): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_29): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_30): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_31): EncoderBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=1280, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize(size=(150,150)),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "JQazjLKEy3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tez_in_pol_Dataset(Dataset):\n",
        "    def __init__(self, path, transform = None):\n",
        "        self.path_list = list(path.glob('*/*.jpg'))\n",
        "        self.transform = transform\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(self.path_list[index])\n",
        "        label = (self.path_list[index]).parts[-2]\n",
        "        label = 0 if 'Polis' in label else 1\n",
        "        if transform != None:\n",
        "            img = transform(img)\n",
        "        return img, int(label)\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "metadata": {
        "id": "LaMCCWXTy3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = pathlib.Path('/content/Tez_in_polis/tren')\n",
        "test_path = pathlib.Path('/content/Tez_in_polis/test')"
      ],
      "metadata": {
        "id": "CScZz8KXy3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Tez_in_pol_Dataset(train_path, transform)\n",
        "test_dataset = Tez_in_pol_Dataset(test_path, transform)"
      ],
      "metadata": {
        "id": "0KMpHJnpy3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = list(train_path.glob('*/*.jpg'))\n",
        "path_test=list(test_path.glob('*/*.jpg'))"
      ],
      "metadata": {
        "id": "hV5yaKWry3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_data = DataLoader(test_dataset, batch_size=5000, shuffle=True)"
      ],
      "metadata": {
        "id": "M7NoUFPQy3hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(My_model3.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "jjJr5jzJyM1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15"
      ],
      "metadata": {
        "id": "SwxhtAAeydnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_dataloader, test_dataloader, criterion, optimizer, epochs=3):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.history = {'loss':[],'acc':[],'val_loss':[],'val_acc':[]}\n",
        "    def train_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.train_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.train_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            sum_loss += loss.item()\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "        train_loss = sum_loss/n\n",
        "        train_accuracy = sum_accuracy/n\n",
        "        self.history['loss'].append(train_loss)\n",
        "        self.history['acc'].append(train_accuracy)\n",
        "        return train_loss, train_accuracy\n",
        "    def validation_loop(self):\n",
        "        sum_loss = 0\n",
        "        sum_accuracy = 0\n",
        "        n = len(self.test_dataloader)\n",
        "        for i, (data,label) in enumerate(tqdm(self.test_dataloader)):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, label)\n",
        "            n_corrects = (output.argmax(axis=1)==label).sum().item()\n",
        "            sum_loss += loss.item()\n",
        "            sum_accuracy += n_corrects/label.size(0)\n",
        "        val_loss = sum_loss/n\n",
        "        val_accuracy = sum_accuracy/n\n",
        "        self.history['val_loss'].append(val_loss)\n",
        "        self.history['val_acc'].append(val_accuracy)\n",
        "        return val_loss, val_accuracy\n",
        "    def train(self):\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc = self.train_loop()\n",
        "            val_loss, val_acc = self.validation_loop()\n",
        "            print()\n",
        "            print(f'Epoch[{epoch+1}/{num_epochs}] \\t train_loss: {train_loss:.5f}, train_acc: {train_acc:.2f} \\t val_loss: {val_loss:.5f} \\t val_acc: {val_acc:.2}')\n"
      ],
      "metadata": {
        "id": "1i_OG-9gygKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1 = Trainer(\n",
        "    model = My_model3.to(device),\n",
        "    train_dataloader = train_data,\n",
        "    test_dataloader = test_data,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "nyyZXfQ0yjXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "2D8ZFOXqynDz",
        "outputId": "7efaa615-405c-4a57-c909-b68e9aa6ed58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/48 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-b6c2a72d1df8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-ac49af555166>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-ac49af555166>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Reshape and permute the input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36m_process_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong image height! Expected {self.image_size} but got {h}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong image width! Expected {self.image_size} but got {w}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mn_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Wrong image height! Expected 224 but got 150!"
          ]
        }
      ]
    }
  ]
}